## Annotation Tools Analysis

### Similar service providers
- River​​side.com
- Twitch Chat Collector Tool
- TIB AV-Analytics <-- Most relatable to our project

- If you need full bounding boxes + tracking across video frames → go with CVAT.

- If you want custom workflows, event annotation, or multi-modal data + video → go with Label Studio.

- If you just need something quick/lightweight for smaller jobs → VIA or VIDAT.

- If your annotation is more about timestamps or behaviours (not object boxes) → PythonVideoAnnotator.


Tool	                Video support  	  Bounding-boxes / tracking	        Scale / team	    Ease of setup
CVAT	                  ✅ strong	            ✅ yes (tracking)	            ✅ good      	Moderate
Label Studio	          ✅ good	            ✅ yes (via config)	            ✅ good	        Moderate
VIDAT	                  ✅ video focus	       TBD (basic)	                   Small–med	    Easier
PythonVideoAnnotator	  ✅ video events	     limited bounding boxes	         Small scale	  Easier (Python GUI)
VIA	                    ✅ basic	             bounding boxes/polygons	       Small scale	  Easy

#Learn CVAT
https://www.youtube.com/playlist?list=PL0to7Ng4PuuYQT4eXlHb_oIlq_RPeuasN
https://www.youtube.com/watch?v=yxX_0-zr-2U&list=PLfYPZalDvZDLvFhjuflhrxk_lLplXUqqB

# Running CVAT (Installation)
-> https://github.com/cvat-ai/cvat?tab=readme-ov-file
-> https://docs.cvat.ai/docs/administration/basics/installation/ (for windows 10, complete till step  *Install Google Chrome*)

Run this command in Linux terminal (avoids git clone failure)
-> wget https://github.com/cvat-ai/cvat/archive/refs/tags/v2.49.0.zip -O cvat.zip
-> sudo apt install unzip
-> unzip cvat.zip
-> mv cvat-2.49.0 cvat
-> cd cvat

Eithier follow option 1 or option 2

<---->
OPTION 1:
-> CVAT_VERSION=v2.49.0 docker compose up -d
<---->

<---->
OPTION 2:
-> export CVAT_VERSION=v2.49.0

-> docker pull cvat/server:${CVAT_VERSION}
-> docker pull cvat/ui:${CVAT_VERSION}

-> docker pull redis:7
-> docker pull postgres:14

-> docker compose up -d

<---->

Verify containers
-> docker ps

-> http://localhost:8080

<----  ---->

<---- Rerun the docker instruction

-> start ur VM (in terminal ->wsl)
-> cd ~/cvat
-> docker compose up -d
-> http://localhost:8080

---->


# Production flow for integrating CVAT + YOLOv8 + Python

Video → CVAT (annotation / export) → YOLOv8 (train or predict)
       ↑                                        ↓
       └────── import auto-annotations (model prelabels) ──────┘

This lets you:

Train models from labeled video frames in CVAT.

Run models to pre-annotate or assist labeling.

Iterate automatically (active learning).


<---- Example (Python SDK)

from cvat_sdk import make_client
from cvat_sdk.api_client.enums import ResourceType

CVAT_URL = "http://localhost:8080"
USERNAME = "admin"
PASSWORD = "password"

client = make_client(CVAT_URL, (USERNAME, PASSWORD))

spec = {
    "name": "traffic_video_task",
    "labels": [{"name": "car"}, {"name": "person"}, {"name": "bike"}]
}

task = client.tasks.create_from_data(
    spec,
    resource_type=ResourceType.LOCAL,
    resources=["/path/to/video.mp4"]
)

print("Created video task:", task.id)

This uploads the video and starts frame extraction on the CVAT server. ---->

https://chatgpt.com/share/6912dc8b-aac8-8002-95ca-85f1c660090e

<---- ABOUT CVAT

Annotation format provided by CVAT
{
  "type": "bbox | polygon | mask | polyline | points",
  "label_id": 2,
  "frame": 120,
  "attributes": {},
  "points": [...],
  "z_order": 0
}

Annotation subtypes
| Type             | Use-case              |
| ---------------- | --------------------- |
| **Bounding Box** | Object detection      |
| **Polygon**      | Instance segmentation |
| **Polyline**     | Lanes, contours       |
| **Points**       | Keypoints, landmarks  |
| **Cuboids (3D)** | 3D perception         |
| **Masks**        | Semantic segmentation |
| **Tracks**       | Video tracking        |

Input video format
Videos (MP4, MKV)

CVAT does not store data inside DB; it stores paths, and media stays on:
Local disk (./cvat/data/)
Cloud buckets

Integration of ML with CVAT

Label → Auto-Segment → Propagate → Track → Manual Adjust
Model predicts → convert to CVAT format → upload → correction by annotators

Output Dataset format
Supported formats:

YOLO (v3/v5/v8)
COCO (instances, keypoints)
Pascal VOC
TFRecord
DeepFashion
MOT
LabelMe
ImageNet
OpenImages
Market1501

---->

<---- IMPLEMENTING CVAT

## Building CVAT using own frontend and backend using only CVAT as an annotation Engine

| Component           | Built by You | Built by CVAT |
| ------------------- | ------------ | ------------- |
| User management     | ✔            | ✔             |
| Upload portal       | ✔            |               |
| Custom UI           | ✔            |               |
| Media handling      | ✔            |               |
| Annotation Canvas   | Optional     |               |
| Annotation storage  | CVAT         | ✔             |
| Task/job assignment | ✔            | ✔             |
| Data export         | ✔ via API    | ✔             |


High Level Architecture
                   +----------------------+
                   |     Your Frontend    |
                   |  (React/Angular/etc) |
                   +----------+-----------+
                              |
                              v
                    (Your REST API / Backend)
                    +---------------------+
                    |  Django/FastAPI     |
                    |  Node/Express       |
                    +----------+----------+
                              |
         -------------------------------------------------
         |                      |                        |
         v                      v                        v
+----------------+   +-------------------+    +----------------------+
|  CVAT Backend  |   |  Your Database    |    |   Cloud Storage      |
|  (Django API)  |   |  (Users, metadata)|    |  (Images/Videos)     |
+----------------+   +-------------------+    +----------------------+


## Use standard Docker compose ->docker compose -f docker-compose.yml up -d

This gives:
CVAT backend API
Redis (job queue)
PostgreSQL
CVAT UI (you can choose to ignore UI)
Nginx proxy

CVAT Backend API: http://<cvat-url>/api/

## To ignore/disable CVAT UI

Remove / comment out in docker-compose.yml:
cvat_ui:
  ...

or
don't give users access to the CVAT UI URL

## Backend Communication with CVAT API

You authenticate with CVAT using:

Basic Auth
Token
Client credentials

-> POST /api/auth/login

## Backend creates tasks
Backend receives a request like:
POST /mybackend/create-task

Backend then calls CVAT:
POST http://cvat/api/tasks
Body:
{
  "name": "video_task_001",
  "project_id": 12
}

## uploading video to CVAT
Backend uploads the media directly:
POST /api/tasks/{id}/data
    data: { } 
    file: [...images/videos]

OR
CVAT reads from cloud storage
CVAT supports:

AWS S3
GCP Storage
OCI bucket
Azure Blob

POST /api/tasks/{id}/data
{
  "remote_files": [
    "s3://bucket/folder/xyz.mp4"
  ]
}

## Assign jobs to annotators
Backend manages annotators.
Each annotator has a CVAT user account created by backend:
POST /api/users
{
  "username": "annotator1",
  "password": "123456"
}

Assign a job:
PATCH /api/jobs/{id}
{
  "assignee": 2
}

## Annotator works in custom frontend
Rebuild the Annotation UI

If you want your UI, you must use:
Frame fetch API:
-> GET /api/jobs/{id}/data?frame=0

Annotation read/write:
-> GET /api/jobs/{id}/annotations
-> PUT /api/jobs/{id}/annotations

Display frames manually
Draw bounding boxes/polygons using your own canvas
Save annotations via PUT

## Export the annotations for training
-> GET /api/tasks/{id}/annotations?format=COCO

Then deliver the file to the ML pipeline.

## Extra Feature: Automatic labeling
Backend can connect your ML models to CVAT through:

CVAT serverless functions
Custom detector API
Triton inference server
YOLO/Detectron2 endpoints

POST /api/serverless/functions/detector/invoke
{
  "task_id": 92
}

CVAT writes annotations automatically → annotators fix them.

## CVAT API endpoints to use
| Purpose            | Endpoint                                  |
| ------------------ | ----------------------------------------- |
| Create task        | `/api/tasks`                              |
| Upload data        | `/api/tasks/{id}/data`                    |
| Create user        | `/api/users`                              |
| Assign job         | `/api/jobs/{id}`                          |
| Fetch frame        | `/api/jobs/{id}/data?frame=N`             |
| Save annotations   | `/api/jobs/{id}/annotations`              |
| Export annotations | `/api/tasks/{id}/annotations?format=COCO` |

